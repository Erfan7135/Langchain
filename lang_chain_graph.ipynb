{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMxsVM+sr8sbhnODw2dCvDQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Erfan7135/Langchain/blob/main/lang_chain_graph.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "KlycsQ_aldeZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i47mljSJmWkx",
        "outputId": "2b937fb3-5f1e-46fb-fe85-6582688e251c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m152.4/152.4 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m40.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.8/43.8 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.0/50.0 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m216.5/216.5 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-generativeai 0.8.5 requires google-ai-generativelanguage==0.6.15, but you have google-ai-generativelanguage 0.6.18 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -q langchain langchain-google-genai langgraph"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "os.environ[\"GOOGLE_API_KEY\"]=userdata.get('GOOGLE_API_KEY')"
      ],
      "metadata": {
        "id": "buR4cPJFmjfV"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**import os:** This module provides a way of using operating system dependent functionality, like setting environment variables.\n",
        "\n",
        "**from google.colab import userdata:** This is a Colab-specific feature to securely access secrets you've stored in the Colab environment.\n",
        "\n",
        "**os.environ[\"GOOGLE_API_KEY\"] = userdata.get('GOOGLE_API_KEY'):** This line retrieves your API key from Colab's secret manager (where you should have saved it as GOOGLE_API_KEY) and sets it as an environment variable. LangChain and langchain-google-genai will automatically pick it up from here."
      ],
      "metadata": {
        "id": "p61-QLc7m81i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Langchain Fundamentals - The Building Blocks**"
      ],
      "metadata": {
        "id": "vZldF3_No2u8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.1 LLMS"
      ],
      "metadata": {
        "id": "o7Ev1NPvdLA6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_core.messages import HumanMessage\n",
        "\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\")\n",
        "\n",
        "response = llm.invoke(\"1+1=?\")\n",
        "\n",
        "print(response.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OwEPgZTHm-Os",
        "outputId": "5dd9e4a5-9c57-43a1-a093-b9a4dc5443c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1+1 = 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.2 Prompts & Prompts Templates"
      ],
      "metadata": {
        "id": "iBDTXx5vdSby"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import PromptTemplate\n",
        "\n",
        "prompt_template = PromptTemplate.from_template(\n",
        "    \"Tell me a {adjective} story about a {noun} within 50 words\"\n",
        ")\n",
        "\n",
        "template = prompt_template.format(adjective=\"funny\",noun=\"cat\")\n",
        "\n",
        "response = llm.invoke(template)\n",
        "\n",
        "print(response.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "xJI_rU_-pPfz",
        "outputId": "0e826a1c-5107-40a6-bb28-459838d3d890"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Whiskers, a fluffy ginger cat, chased the red laser dot with fierce determination. It bounced, it wiggled! Suddenly, the dot landed square on his human's forehead. Whiskers leaped, headbutting his owner with a triumphant \"Mrow!\" The human groaned. Whiskers, victorious, then napped on the remote, mission accomplished.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.3 Chains"
      ],
      "metadata": {
        "id": "XFqxF8mcqPY0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "# A more advanced prompt template specifically for chat models\n",
        "chat_prompt_template = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You are a helpful AI assistant that translates text.\"),\n",
        "    (\"user\", \"Translate the following text into {language}: {text}\")\n",
        "])\n",
        "\n",
        "translation_chain = chat_prompt_template | llm | StrOutputParser()\n",
        "# LCEL -> langchain expression language -> compose runnable(?)\n",
        "\n",
        "result = translation_chain.invoke({\"language\": \"bangla\", \"text\": \"I love programming.\"})\n",
        "\n",
        "print (result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iCIcUWk9qVdC",
        "outputId": "af53e154-7a0b-4f5a-90dd-3343d83ff1b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "আমি প্রোগ্রামিং ভালোবাসি।\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.4 Output Parsers"
      ],
      "metadata": {
        "id": "qhT0SD96rlT7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.output_parsers import CommaSeparatedListOutputParser\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "\n",
        "list_parser = CommaSeparatedListOutputParser()\n",
        "\n",
        "format_instructions = list_parser.get_format_instructions()\n",
        "#this formate intruction is generated to pass into ai to get dformatted output\n",
        "print(format_instructions)\n",
        "\n",
        "prompt = PromptTemplate.from_template(\n",
        "    \"List five {subject}.\\n{format_instructions}\"\n",
        ")\n",
        "\n",
        "list_chain = prompt | llm | list_parser\n",
        "\n",
        "result = list_chain.invoke({\"subject\": \"ice cream flavors\", \"format_instructions\": format_instructions})\n",
        "\n",
        "print (result)\n",
        "print(type(result))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DxVoCyLnqshM",
        "outputId": "d5ac20dc-11d4-43de-e599-90d69f75bf38"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your response should be a list of comma separated values, eg: `foo, bar, baz` or `foo,bar,baz`\n",
            "['Vanilla', 'Chocolate', 'Strawberry', 'Mint Chocolate Chip', 'Cookies and Cream']\n",
            "<class 'list'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Assignment"
      ],
      "metadata": {
        "id": "jUCtAoyHdlJM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Your First Assignment:**\n",
        "Your first task is to combine these concepts.\n",
        "\n",
        "Task: Create a LangChain chain that:\n",
        "\n",
        "*   Takes a user_query (e.g., \"Summarize this article: [article text]\").\n",
        "*   Uses a ChatPromptTemplate to instruct the Gemini model to summarize the provided text.\n",
        "*   Pipes the prompt to the llm (your ChatGoogleGenerativeAI instance).\n",
        "Uses StrOutputParser to get the clean summary.\n",
        "*   Invoke this chain with a sample article text and print the summary."
      ],
      "metadata": {
        "id": "4dPtqpeCuANE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\")\n",
        "\n",
        "template = ChatPromptTemplate.from_template(\n",
        "    \"Summarize this article: {article_text}\"\n",
        ")\n",
        "\n",
        "summary_chain = template | llm | StrOutputParser()\n",
        "\n",
        "article_text = \"\"\"\n",
        "The Amazon rainforest is the largest rainforest in the world, covering an area of about 6 million square kilometers.\n",
        "It spans across nine countries, with the majority of it located in Brazil.\n",
        "The rainforest is incredibly biodiverse, home to millions of species of plants, animals, and insects, many of which are unique to the region.\n",
        "It plays a crucial role in regulating the Earth's climate by absorbing vast amounts of carbon dioxide and producing oxygen.\n",
        "However, the Amazon is under threat from deforestation, primarily due to agricultural expansion, logging, and mining, which has significant implications for global climate change and biodiversity loss.\n",
        "Efforts are being made to conserve the rainforest, but challenges remain.\n",
        "\"\"\"\n",
        "summery = summary_chain.invoke({\"article_text\": article_text})\n",
        "\n",
        "print(summery)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AyF2MD_guXoj",
        "outputId": "473f2722-a315-47b8-bdef-e5d0da0d1c6c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Amazon rainforest, the world's largest, covers about 6 million square kilometers across nine countries, primarily Brazil. It is highly biodiverse and vital for global climate regulation by absorbing carbon dioxide and producing oxygen. However, it faces severe threats from deforestation due driven by agricultural expansion, logging, and mining, which has significant implications for global climate change and biodiversity loss, despite ongoing conservation efforts.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **LangGraph for State & Agentic Systems**\n"
      ],
      "metadata": {
        "id": "XlSUlun3eczZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## *2.1 Graph Fundamentals - Nodes & Edges*"
      ],
      "metadata": {
        "id": "ZT7ysMgtecD3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.graph import StateGraph, END\n",
        "from typing import TypedDict, Annotated\n",
        "import operator\n",
        "from langchain_core.messages import BaseMessage, HumanMessage\n",
        "\n",
        "#state\n",
        "class AgentState(TypedDict):\n",
        "  messages : Annotated[list[BaseMessage], operator.add]\n",
        "\n",
        "#graph\n",
        "workflow = StateGraph(AgentState)\n",
        "\n",
        "#node\n",
        "def say_hello(state : AgentState):\n",
        "  print(\"Executing 'say_hello' node...\")\n",
        "  current_messages = state.get(\"messages\", [])\n",
        "  new_message = HumanMessage(content=\"Hello from the graph\")\n",
        "  current_messages.append(new_message)\n",
        "  # return {\"messages\": current_messages}\n",
        "\n",
        "def say_goodbye(state : AgentState):\n",
        "  print(\"Executing 'say_goodbye' node...\")\n",
        "  current_messages = state.get(\"messages\", [])\n",
        "  new_message = HumanMessage(content=\"Goodbye from the graph\")\n",
        "  current_messages.append(new_message)\n",
        "  # return {\"messages\": current_messages}\n",
        "\n",
        "#Add node to workflow\n",
        "workflow.add_node(\"node_hello\", say_hello)\n",
        "workflow.add_node(\"node_goodbye\", say_goodbye)\n",
        "\n",
        "workflow.set_entry_point(\"node_hello\")\n",
        "workflow.add_edge(\"node_hello\", \"node_goodbye\")\n",
        "workflow.add_edge(\"node_goodbye\", END)\n",
        "\n",
        "app = workflow.compile()\n",
        "\n",
        "print(\"---Running the simple graph---\")\n",
        "initial_state = {\"messages\": []}\n",
        "final_state = app.invoke(initial_state)\n",
        "\n",
        "for msg in final_state[\"messages\"]:\n",
        "  print(f\"[{msg.type.upper()}]: {msg.content}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AUArT9rpfACg",
        "outputId": "a9f002eb-ff90-4ab5-d71b-8303ee60ea57"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---Running the simple graph---\n",
            "Executing 'say_hello' node...\n",
            "Executing 'say_goodbye' node...\n",
            "[HUMAN]: Hello from the graph\n",
            "[HUMAN]: Goodbye from the graph\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2 Building LLM Agent"
      ],
      "metadata": {
        "id": "ugBvOERAfAVm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2.1 Defining Simple Tool"
      ],
      "metadata": {
        "id": "jIYVBIp-yGFQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from langchain_core.tools import tool\n",
        "from datetime import datetime\n",
        "\n",
        "@tool\n",
        "def get_current_datetime() -> str:\n",
        "  \"\"\"Returns the current date and time.\"\"\"\n",
        "  return \"Saturday, June 21, 2025 at 1:17:22 AM +06\"\n",
        "\n",
        "@tool\n",
        "def get_current_weather(location: str) -> str:\n",
        "  \"\"\"Gets the current weather for a specified location.\"\"\"\n",
        "  if \"dhaka\" in location.lower():\n",
        "    return \"It's 30 degrees Celsius and sunny in Dhaka. High humidity.\"\n",
        "  elif \"london\" in location.lower():\n",
        "      return \"It's 15 degrees Celsius and cloudy in London.\"\n",
        "  else:\n",
        "    return \"Weather data not available for this location.\"\n",
        "\n",
        "print(\"---Tools Defined---\")\n",
        "print(f\"Tool 1 name : {get_current_datetime.name}\")\n",
        "print(f\"Tool 1 description : {get_current_datetime.description}\")\n",
        "print(f\"Tool 2 name : {get_current_weather.name}\")\n",
        "print(f\"Tool 2 description : {get_current_weather.description}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DpGMiTJZxvvI",
        "outputId": "6c0635f8-6f73-453c-c31c-d826b8a8cde1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---Tools Defined---\n",
            "Tool 1 name : get_current_datetime\n",
            "Tool 1 description : Returns the current date and time.\n",
            "Tool 2 name : get_current_weather\n",
            "Tool 2 description : Gets the current weather for a specified location.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2.2 Teaching LLM about Tools"
      ],
      "metadata": {
        "id": "vVB1PMgezUBI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "import os\n",
        "from langchain_core.tools import tool\n",
        "from datetime import datetime\n",
        "\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\")\n",
        "\n",
        "my_tools = [get_current_datetime, get_current_weather]\n",
        "\n",
        "llm_with_tools = llm.bind_tools(my_tools)\n",
        "\n",
        "print(\"--- LLM with Tools Initialized ---\")\n",
        "print(\"We have bound our tools to the LLM. Now it knows about them.\")\n",
        "\n",
        "from langchain_core.messages import HumanMessage, AIMessage\n",
        "\n",
        "print(\"\\n--- Invoking LLM to ask for weather ---\")\n",
        "# The LLM doesn't execute the tool here, it just *decides* to call it.\n",
        "# Its response will contain the tool call suggestion.\n",
        "response_weather = llm_with_tools.invoke([HumanMessage(content=\"What's the weather in Dhaka?\")])\n",
        "print(f\"LLM's raw response for weather query:\")\n",
        "print(f\"Content: {response_weather.content}\")\n",
        "print(f\"Tool Calls: {response_weather.tool_calls}\")\n",
        "\n",
        "print(\"\\n--- Invoking LLM to ask for current date and time ---\")\n",
        "response_datetime = llm_with_tools.invoke([HumanMessage(content=\"What is the current date and time?\")])\n",
        "print(f\"LLM's raw response for datetime query:\")\n",
        "print(f\"Content: {response_datetime.content}\")\n",
        "print(f\"Tool Calls: {response_datetime.tool_calls}\")\n",
        "\n",
        "\n",
        "print(\"\\n--- Invoking LLM for a general question (should NOT call a tool) ---\")\n",
        "response_joke = llm_with_tools.invoke([HumanMessage(content=\"Tell me a short joke about a dog.\")])\n",
        "print(f\"LLM's raw response for joke query:\")\n",
        "print(f\"Content: {response_joke.content}\")\n",
        "print(f\"Tool Calls: {response_joke.tool_calls}\") # This should be an empty list!"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "-Niz1DRXzmtA",
        "outputId": "b799b4e7-ac9e-4557-cbc8-557520141a08"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'get_current_weather' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4-773810690.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mllm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mChatGoogleGenerativeAI\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"gemini-2.5-flash\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mmy_tools\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mget_current_datetime\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_current_weather\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mllm_with_tools\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mllm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind_tools\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmy_tools\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'get_current_weather' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2.3 Executing Tools and Feeding Results Back"
      ],
      "metadata": {
        "id": "TuqhB5thG01Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from langchain_core.tools import tool\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langgraph.graph import StateGraph, END # We'll use END later, just import it now\n",
        "from typing import TypedDict, Annotated\n",
        "import operator\n",
        "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, ToolMessage\n",
        "from datetime import datetime\n",
        "\n",
        "# Define our tools again for this section\n",
        "@tool\n",
        "def get_current_datetime() -> str:\n",
        "    \"\"\"Returns the current date and time.\"\"\"\n",
        "    return datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "\n",
        "# Bundle and bind tools\n",
        "tools = [get_current_datetime]\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\", temperature=0.0)\n",
        "llm_with_tools = llm.bind_tools(tools)\n",
        "tool_map = {tool.name: tool for tool in tools} # For looking up\n",
        "\n",
        "class AgentState(TypedDict):\n",
        "    messages: Annotated[list[BaseMessage], operator.add]\n",
        "\n",
        "def call_llm_node(state: AgentState):\n",
        "    print(\"Executing 'call_llm_node'...\")\n",
        "    messages = state[\"messages\"]\n",
        "    response = llm_with_tools.invoke(messages)\n",
        "    print(f\"LLMs response : {response}\")\n",
        "    return {\"messages\": [response]}\n",
        "\n",
        "def execute_tool_node(state: AgentState):\n",
        "    print(\"Executing 'call_tool_node'...\")\n",
        "    messages = state[\"messages\"]\n",
        "    tools_called = messages[-1].tool_calls\n",
        "\n",
        "    if not tools_called:\n",
        "        return {\"messages\": []}\n",
        "\n",
        "    tools_output = []\n",
        "    for tool_call in tools_called:\n",
        "        tool_name = tool_call['name']\n",
        "        tool_args = tool_call['args']\n",
        "        tool_call_id = tool_call['id']\n",
        "\n",
        "        print(f\"Attempting to execute tool: {tool_name}\")\n",
        "\n",
        "        if tool_name in tool_map:\n",
        "            called_tool = tool_map[tool_name]\n",
        "            try:\n",
        "              output = called_tool.invoke(tool_args)\n",
        "              tools_output.append(ToolMessage(content=output, tool_call_id=tool_call_id))\n",
        "            except Exception as e:\n",
        "              error_message = f\"Error executing tool {tool_name}: {str(e)}\"\n",
        "              print(error_message)\n",
        "              tools_output.append(ToolMessage(content=error_message, tool_call_id=tool_call_id))\n",
        "        else:\n",
        "            error_message = f\"Error: Unknown tool called: {tool_name}\"\n",
        "            print(error_message)\n",
        "            tools_output.append(ToolMessage(content=error_message, tool_call_id=tool_call_id))\n",
        "\n",
        "    print(f\"  Tool outputs to add to state: {tools_output}\")\n",
        "    return {\"messages\": tools_output} # Return the list of ToolMessage objects\n",
        "\n",
        "\n",
        "workflow = StateGraph(AgentState)\n",
        "\n",
        "workflow.add_node(\"llm_node\", call_llm_node)\n",
        "workflow.add_node(\"tool_node\", execute_tool_node)\n",
        "\n",
        "workflow.set_entry_point(\"llm_node\")\n",
        "\n",
        "def decide_next_node(state: AgentState):\n",
        "    print(\"Executing 'decide_next_node'...\")\n",
        "    messages = state[\"messages\"]\n",
        "    last_message = messages[-1]\n",
        "    if last_message.tool_calls:\n",
        "        print(\"Decision: Go to 'tool_node' (LLM wants to use a tool)\")\n",
        "        return \"tool\"\n",
        "    else:\n",
        "        print(\"Decision: End (LLM provided final answer)\")\n",
        "        return \"end\"\n",
        "\n",
        "workflow.add_conditional_edges(\n",
        "    \"llm_node\",\n",
        "    decide_next_node,\n",
        "    {\n",
        "        \"tool\": \"tool_node\",\n",
        "        \"end\": END\n",
        "    }\n",
        ")\n",
        "\n",
        "workflow.add_edge(\"tool_node\", \"llm_node\")\n",
        "\n",
        "app = workflow.compile()\n",
        "\n",
        "print(\"\\n===== TEST 1: Query requiring a tool =====\")\n",
        "user_query_tool = \"What is the current date and time?\"\n",
        "initial_state_tool = {\"messages\": [HumanMessage(content=user_query_tool)]}\n",
        "\n",
        "print(f\"\\n--- Initial User Query: '{user_query_tool}' ---\")\n",
        "# Use stream to see what happens at each step (node execution)\n",
        "for s in app.stream(initial_state_tool):\n",
        "    print(s)\n",
        "    if list(s.values()) and 'messages' in list(s.values())[0]:\n",
        "        final_state_weather = list(s.values())[0]\n",
        "\n",
        "# To see the final full conversation:\n",
        "final_conversation_tool = app.invoke(initial_state_tool)\n",
        "print(\"\\n--- Final Conversation History (Test 1) ---\")\n",
        "for msg in final_conversation_tool['messages']:\n",
        "    print(f\"[{msg.__class__.__name__}]: {msg.content}\")\n",
        "    if hasattr(msg, 'tool_calls') and msg.tool_calls:\n",
        "        print(f\"  Tool Calls: {msg.tool_calls}\")\n",
        "    if hasattr(msg, 'tool_call_id') and msg.tool_call_id:\n",
        "        print(f\"  Tool Call ID: {msg.tool_call_id}\")\n",
        "\n",
        "\n",
        "print(\"\\n\\n===== TEST 2: Query NOT requiring a tool =====\")\n",
        "user_query_no_tool = \"Tell me a fun fact about giraffes.\"\n",
        "initial_state_no_tool = {\"messages\": [HumanMessage(content=user_query_no_tool)]}\n",
        "\n",
        "print(f\"\\n--- Initial User Query: '{user_query_no_tool}' ---\")\n",
        "for s in app.stream(initial_state_no_tool):\n",
        "    print(s)\n",
        "\n",
        "final_conversation_no_tool = app.invoke(initial_state_no_tool)\n",
        "print(\"\\n--- Final Conversation History (Test 2) ---\")\n",
        "for msg in final_conversation_no_tool['messages']:\n",
        "    print(f\"[{msg.__class__.__name__}]: {msg.content}\")\n",
        "    if hasattr(msg, 'tool_calls') and msg.tool_calls:\n",
        "        print(f\"  Tool Calls: {msg.tool_calls}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "PNGBCskDG1IP",
        "outputId": "2940a74c-e406-4846-e7fd-3ab40c68f721"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== TEST 1: Query requiring a tool =====\n",
            "\n",
            "--- Initial User Query: 'What is the current date and time?' ---\n",
            "Executing 'call_llm_node'...\n",
            "LLMs response : content='' additional_kwargs={'function_call': {'name': 'get_current_datetime', 'arguments': '{}'}} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []} id='run--7ff6c6f4-ee85-4525-9af6-b3404775f8eb-0' tool_calls=[{'name': 'get_current_datetime', 'args': {}, 'id': '5efc9bec-97f7-44f8-8911-27fc588b85da', 'type': 'tool_call'}] usage_metadata={'input_tokens': 39, 'output_tokens': 12, 'total_tokens': 90, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 39}}\n",
            "Executing 'decide_next_node'...\n",
            "Decision: Go to 'tool_node' (LLM wants to use a tool)\n",
            "{'llm_node': {'messages': [AIMessage(content='', additional_kwargs={'function_call': {'name': 'get_current_datetime', 'arguments': '{}'}}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []}, id='run--7ff6c6f4-ee85-4525-9af6-b3404775f8eb-0', tool_calls=[{'name': 'get_current_datetime', 'args': {}, 'id': '5efc9bec-97f7-44f8-8911-27fc588b85da', 'type': 'tool_call'}], usage_metadata={'input_tokens': 39, 'output_tokens': 12, 'total_tokens': 90, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 39}})]}}\n",
            "Executing 'call_tool_node'...\n",
            "Attempting to execute tool: get_current_datetime\n",
            "  Tool outputs to add to state: [ToolMessage(content='2025-06-21 12:59:55', tool_call_id='5efc9bec-97f7-44f8-8911-27fc588b85da')]\n",
            "{'tool_node': {'messages': [ToolMessage(content='2025-06-21 12:59:55', tool_call_id='5efc9bec-97f7-44f8-8911-27fc588b85da')]}}\n",
            "Executing 'call_llm_node'...\n",
            "LLMs response : content='The current date and time is 2025-06-21 12:59:55.' additional_kwargs={} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []} id='run--ca687c97-1737-44db-839c-ca45d5d5ea94-0' usage_metadata={'input_tokens': 86, 'output_tokens': 27, 'total_tokens': 113, 'input_token_details': {'cache_read': 0}}\n",
            "Executing 'decide_next_node'...\n",
            "Decision: End (LLM provided final answer)\n",
            "{'llm_node': {'messages': [AIMessage(content='The current date and time is 2025-06-21 12:59:55.', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []}, id='run--ca687c97-1737-44db-839c-ca45d5d5ea94-0', usage_metadata={'input_tokens': 86, 'output_tokens': 27, 'total_tokens': 113, 'input_token_details': {'cache_read': 0}})]}}\n",
            "Executing 'call_llm_node'...\n",
            "LLMs response : content='' additional_kwargs={'function_call': {'name': 'get_current_datetime', 'arguments': '{}'}} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []} id='run--2e685e27-483f-489d-8786-96fe94cd988d-0' tool_calls=[{'name': 'get_current_datetime', 'args': {}, 'id': 'a9df2d7b-40c8-40b0-b350-47457bbcb8bc', 'type': 'tool_call'}] usage_metadata={'input_tokens': 39, 'output_tokens': 12, 'total_tokens': 90, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 39}}\n",
            "Executing 'decide_next_node'...\n",
            "Decision: Go to 'tool_node' (LLM wants to use a tool)\n",
            "Executing 'call_tool_node'...\n",
            "Attempting to execute tool: get_current_datetime\n",
            "  Tool outputs to add to state: [ToolMessage(content='2025-06-21 12:59:56', tool_call_id='a9df2d7b-40c8-40b0-b350-47457bbcb8bc')]\n",
            "Executing 'call_llm_node'...\n",
            "LLMs response : content='The current date and time is 2025-06-21 12:59:56.' additional_kwargs={} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []} id='run--5545feb4-ed94-4527-8813-dd1310c84d4b-0' usage_metadata={'input_tokens': 86, 'output_tokens': 27, 'total_tokens': 113, 'input_token_details': {'cache_read': 0}}\n",
            "Executing 'decide_next_node'...\n",
            "Decision: End (LLM provided final answer)\n",
            "\n",
            "--- Final Conversation History (Test 1) ---\n",
            "[HumanMessage]: What is the current date and time?\n",
            "[AIMessage]: \n",
            "  Tool Calls: [{'name': 'get_current_datetime', 'args': {}, 'id': 'a9df2d7b-40c8-40b0-b350-47457bbcb8bc', 'type': 'tool_call'}]\n",
            "[ToolMessage]: 2025-06-21 12:59:56\n",
            "  Tool Call ID: a9df2d7b-40c8-40b0-b350-47457bbcb8bc\n",
            "[AIMessage]: The current date and time is 2025-06-21 12:59:56.\n",
            "\n",
            "\n",
            "===== TEST 2: Query NOT requiring a tool =====\n",
            "\n",
            "--- Initial User Query: 'Tell me a fun fact about giraffes.' ---\n",
            "Executing 'call_llm_node'...\n",
            "LLMs response : content=\"Did you know that giraffes have the same number of vertebrae in their necks as humans? They both have seven, but a giraffe's vertebrae are just much, much longer!\" additional_kwargs={} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []} id='run--f2c18e29-68dc-4ab1-9e27-99db6cc4d963-0' usage_metadata={'input_tokens': 40, 'output_tokens': 36, 'total_tokens': 114, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 38}}\n",
            "Executing 'decide_next_node'...\n",
            "Decision: End (LLM provided final answer)\n",
            "{'llm_node': {'messages': [AIMessage(content=\"Did you know that giraffes have the same number of vertebrae in their necks as humans? They both have seven, but a giraffe's vertebrae are just much, much longer!\", additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []}, id='run--f2c18e29-68dc-4ab1-9e27-99db6cc4d963-0', usage_metadata={'input_tokens': 40, 'output_tokens': 36, 'total_tokens': 114, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 38}})]}}\n",
            "Executing 'call_llm_node'...\n",
            "LLMs response : content=\"Did you know that giraffes have the same number of vertebrae in their necks as humans? They both have seven, but a giraffe's vertebrae are just much, much longer!\" additional_kwargs={} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []} id='run--2a697d46-319c-4d70-9f79-0ffdf08b590d-0' usage_metadata={'input_tokens': 40, 'output_tokens': 36, 'total_tokens': 114, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 38}}\n",
            "Executing 'decide_next_node'...\n",
            "Decision: End (LLM provided final answer)\n",
            "\n",
            "--- Final Conversation History (Test 2) ---\n",
            "[HumanMessage]: Tell me a fun fact about giraffes.\n",
            "[AIMessage]: Did you know that giraffes have the same number of vertebrae in their necks as humans? They both have seven, but a giraffe's vertebrae are just much, much longer!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2.4 The Full Agent Loop"
      ],
      "metadata": {
        "id": "aJNuvh3nxv_3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from langchain_core.tools import tool\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langgraph.graph import StateGraph, END\n",
        "from typing import Annotated, TypedDict\n",
        "import operator\n",
        "from langchain_core.messages import HumanMessage, AIMessage, BaseMessage, ToolMessage\n",
        "from datetime import datetime\n",
        "\n",
        "#--- 1. Define Tools ---\n",
        "@tool\n",
        "def get_current_datetime():\n",
        "  \"\"\"Returns the current Date & Time\"\"\"\n",
        "  return datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "\n",
        "@tool\n",
        "def get_current_weather(location: str) -> str:\n",
        "  \"\"\"Gets the current weather for a speicified location\"\"\"\n",
        "  if \"dhaka\" in location.lower():\n",
        "    return \"As of 2025-06-21 13:45:34: It's 30 degrees Celsius and sunny in Dhaka. High humidity.\"\n",
        "  elif \"london\" in location.lower():\n",
        "      return \"It's 15 degrees Celsius and cloudy in London.\"\n",
        "  else:\n",
        "      return \"Weather data not available for this location.\"\n",
        "\n",
        "tools = [get_current_datetime, get_current_weather]\n",
        "tool_map = {tool.name: tool for tool in tools}\n",
        "\n",
        "# --- 2. Initialized llm tools ---\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\", temperature=0.0)\n",
        "llm_with_tools = llm.bind_tools(tools)\n",
        "\n",
        "# --- 3. Define Agent State ---\n",
        "class AgentState(TypedDict):\n",
        "  messages : Annotated[list[BaseMessage], operator.add]\n",
        "\n",
        "# --- 4. Define Nodes ---\n",
        "\n",
        "def call_llm_node(state : AgentState):\n",
        "  print(\"Executing 'call_llm_node'...\")\n",
        "  messages = state[\"messages\"]\n",
        "  response = llm_with_tools.invoke(messages)\n",
        "  return {\"messages\": [response]}\n",
        "\n",
        "def execute_tool_node(state : AgentState):\n",
        "  print(\"Executing 'execute_tool_node'...\")\n",
        "  messages = state[\"messages\"]\n",
        "  tools_called = messages[-1].tool_calls\n",
        "\n",
        "  if not tools_called:\n",
        "    print(\"No tool calls to execute\")\n",
        "    return {\"messages\": []}\n",
        "\n",
        "  tools_output = []\n",
        "\n",
        "  for tool_call in tools_called:\n",
        "    tool_name = tool_call['name']\n",
        "    tool_args = tool_call['args']\n",
        "    tool_call_id = tool_call['id']\n",
        "\n",
        "    print(\"Attempting to executing tool :{tool_name}\")\n",
        "\n",
        "    if tool_name in tool_map:\n",
        "      called_tool = tool_map[tool_name]\n",
        "      try:\n",
        "        output = called_tool.invoke(tool_args)\n",
        "        tools_output.append(ToolMessage(content=output, tool_call_id=tool_call_id))\n",
        "      except Exception as e:\n",
        "        error_message = f\"Error executing tool {tool_name}: {str(e)}\"\n",
        "        print(error_message)\n",
        "        tools_output.append(ToolMessage(content=error_message, tool_call_id=tool_call_id))\n",
        "    else:\n",
        "      error_message = f\"Error: Unknown tool called: {tool_name}\"\n",
        "      print(error_message)\n",
        "      tools_output.append(ToolMessage(content=error_message, tool_call_id=tool_call_id))\n",
        "\n",
        "  print(f\"    Tool outputs to add to state: {tools_output}\")\n",
        "  return {\"messages\": tools_output}\n",
        "\n",
        "# --- 5. Define Conditional Edge Logic (The Decision Maker)---\n",
        "def decide_next_step(state : AgentState):\n",
        "  print(\"Executing 'decide_next_step'...\")\n",
        "  messages = state[\"messages\"]\n",
        "  last_message = messages[-1]\n",
        "\n",
        "  if last_message.tool_calls:\n",
        "    print(\"Decision: Go to 'tool_node' (LLM wants to use a tool)\")\n",
        "    return \"tool\"\n",
        "  else:\n",
        "    print(\"Decision: End (LLM provided final answer)\")\n",
        "    return \"end\"\n",
        "\n",
        "# --- 6. Build  the LangGraph workflow ---\n",
        "\n",
        "workflow = StateGraph(AgentState)\n",
        "workflow.add_node(\"llm_node\", call_llm_node)\n",
        "workflow.add_node(\"tool_node\", execute_tool_node)\n",
        "\n",
        "workflow.set_entry_point(\"llm_node\")\n",
        "\n",
        "workflow.add_conditional_edges(\n",
        "    \"llm_node\",\n",
        "    decide_next_step,\n",
        "    {\n",
        "        \"tool\": \"tool_node\",\n",
        "        \"end\": END\n",
        "    }\n",
        ")\n",
        "workflow.add_edge(\"tool_node\", \"llm_node\")\n",
        "\n",
        "# --- 7. Compile The Graph ---\n",
        "app = workflow.compile()\n",
        "\n",
        "# --- 8. TestCases ---\n",
        "print(\"\\n===== Running Test Case 1: Query requiring 'get_current_datetime' tool =====\")\n",
        "user_query_datetime = \"What is the current date and time?\"\n",
        "initial_state_datetime = {\"messages\": [HumanMessage(content=user_query_datetime)]}\n",
        "\n",
        "print(f\"\\n--- User Query: '{user_query_datetime}' ---\")\n",
        "final_state_datetime = {}\n",
        "for s in app.stream(initial_state_datetime):\n",
        "    print(s)\n",
        "    # The 's' dictionary contains {node_name: {state_update}}.\n",
        "    # We want to capture the full state from the last step.\n",
        "    # The 'messages' list is inside the value of the 's' dictionary.\n",
        "    if list(s.values()) and 'messages' in list(s.values())[0]:\n",
        "        final_state_datetime = list(s.values())[0]\n",
        "\n",
        "\n",
        "print(\"\\n--- Final Conversation History (Test 1) ---\")\n",
        "if final_state_datetime:\n",
        "    for msg in final_state_datetime['messages']:\n",
        "        print(f\"[{msg.__class__.__name__}]: {msg.content.strip()}\") # .strip() for cleaner output\n",
        "        if hasattr(msg, 'tool_calls') and msg.tool_calls:\n",
        "            print(f\"  Tool Calls: {msg.tool_calls}\")\n",
        "        if hasattr(msg, 'tool_call_id') and msg.tool_call_id:\n",
        "            print(f\"  Tool Call ID: {msg.tool_call_id}\")\n",
        "else:\n",
        "    print(\"No final state captured for Test 1.\")\n",
        "\n",
        "\n",
        "print(\"\\n\\n===== Running Test Case 2: Query requiring 'get_current_weather' tool =====\")\n",
        "user_query_weather = \"How's the weather in London?\"\n",
        "initial_state_weather = {\"messages\": [HumanMessage(content=user_query_weather)]}\n",
        "\n",
        "print(f\"\\n--- User Query: '{user_query_weather}' ---\")\n",
        "final_state_weather = {}\n",
        "for s in app.stream(initial_state_weather):\n",
        "    print(s)\n",
        "    if list(s.values()) and 'messages' in list(s.values())[0]:\n",
        "        final_state_weather = list(s.values())[0]\n",
        "\n",
        "print(\"\\n--- Final Conversation History (Test 2) ---\")\n",
        "if final_state_weather:\n",
        "    for msg in final_state_weather['messages']:\n",
        "        print(f\"[{msg.__class__.__name__}]: {msg.content.strip()}\")\n",
        "        if hasattr(msg, 'tool_calls') and msg.tool_calls:\n",
        "            print(f\"  Tool Calls: {msg.tool_calls}\")\n",
        "        if hasattr(msg, 'tool_call_id') and msg.tool_call_id:\n",
        "            print(f\"  Tool Call ID: {msg.tool_call_id}\")\n",
        "else:\n",
        "    print(\"No final state captured for Test 2.\")\n",
        "\n",
        "\n",
        "print(\"\\n\\n===== Running Test Case 3: Query NOT requiring a tool =====\")\n",
        "user_query_joke = \"Tell me a short joke about an elephant.\"\n",
        "user_query_joke = \"If I go to London from Dhaka, should I bring my umbrella?\"\n",
        "initial_state_joke = {\"messages\": [HumanMessage(content=user_query_joke)]}\n",
        "\n",
        "print(f\"\\n--- User Query: '{user_query_joke}' ---\")\n",
        "final_state_joke = {}\n",
        "for s in app.stream(initial_state_joke):\n",
        "    print(s)\n",
        "    if list(s.values()) and 'messages' in list(s.values())[0]:\n",
        "        final_state_joke = list(s.values())[0]\n",
        "\n",
        "print(\"\\n--- Final Conversation History (Test 3) ---\")\n",
        "if final_state_joke:\n",
        "    for msg in final_state_joke['messages']:\n",
        "        print(f\"[{msg.__class__.__name__}]: {msg.content.strip()}\")\n",
        "        if hasattr(msg, 'tool_calls') and msg.tool_calls:\n",
        "            print(f\"  Tool Calls: {msg.tool_calls}\")\n",
        "else:\n",
        "    print(\"No final state captured for Test 3.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "owQFMVctdC2Y",
        "outputId": "347e6dc2-9142-483a-af39-c02fdae364cc"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== Running Test Case 1: Query requiring 'get_current_datetime' tool =====\n",
            "\n",
            "--- User Query: 'What is the current date and time?' ---\n",
            "Executing 'call_llm_node'...\n",
            "Executing 'decide_next_step'...\n",
            "Decision: Go to 'tool_node' (LLM wants to use a tool)\n",
            "{'llm_node': {'messages': [AIMessage(content='', additional_kwargs={'function_call': {'name': 'get_current_datetime', 'arguments': '{}'}}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []}, id='run--2f80edbb-c270-4344-88a1-0401d2e2d637-0', tool_calls=[{'name': 'get_current_datetime', 'args': {}, 'id': '1b58b445-bd22-4f91-b28a-6d56a698951e', 'type': 'tool_call'}], usage_metadata={'input_tokens': 82, 'output_tokens': 12, 'total_tokens': 133, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 39}})]}}\n",
            "Executing 'execute_tool_node'...\n",
            "Attempting to executing tool :{tool_name}\n",
            "    Tool outputs to add to state: [ToolMessage(content='2025-06-21 13:33:34', tool_call_id='1b58b445-bd22-4f91-b28a-6d56a698951e')]\n",
            "{'tool_node': {'messages': [ToolMessage(content='2025-06-21 13:33:34', tool_call_id='1b58b445-bd22-4f91-b28a-6d56a698951e')]}}\n",
            "Executing 'call_llm_node'...\n",
            "Executing 'decide_next_step'...\n",
            "Decision: End (LLM provided final answer)\n",
            "{'llm_node': {'messages': [AIMessage(content='The current date and time is 2025-06-21 13:33:34.', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []}, id='run--c20e387a-a2b6-4d7c-b650-2dc4c33d76a9-0', usage_metadata={'input_tokens': 129, 'output_tokens': 27, 'total_tokens': 156, 'input_token_details': {'cache_read': 0}})]}}\n",
            "\n",
            "--- Final Conversation History (Test 1) ---\n",
            "[AIMessage]: The current date and time is 2025-06-21 13:33:34.\n",
            "\n",
            "\n",
            "===== Running Test Case 2: Query requiring 'get_current_weather' tool =====\n",
            "\n",
            "--- User Query: 'How's the weather in London?' ---\n",
            "Executing 'call_llm_node'...\n",
            "Executing 'decide_next_step'...\n",
            "Decision: Go to 'tool_node' (LLM wants to use a tool)\n",
            "{'llm_node': {'messages': [AIMessage(content='', additional_kwargs={'function_call': {'name': 'get_current_weather', 'arguments': '{\"location\": \"London\"}'}}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []}, id='run--103789b4-1017-4f55-b53d-7deb76a52140-0', tool_calls=[{'name': 'get_current_weather', 'args': {'location': 'London'}, 'id': '6c0db537-a9b1-40f5-87af-209d04c7c33f', 'type': 'tool_call'}], usage_metadata={'input_tokens': 82, 'output_tokens': 17, 'total_tokens': 160, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 61}})]}}\n",
            "Executing 'execute_tool_node'...\n",
            "Attempting to executing tool :{tool_name}\n",
            "    Tool outputs to add to state: [ToolMessage(content=\"It's 15 degrees Celsius and cloudy in London.\", tool_call_id='6c0db537-a9b1-40f5-87af-209d04c7c33f')]\n",
            "{'tool_node': {'messages': [ToolMessage(content=\"It's 15 degrees Celsius and cloudy in London.\", tool_call_id='6c0db537-a9b1-40f5-87af-209d04c7c33f')]}}\n",
            "Executing 'call_llm_node'...\n",
            "Executing 'decide_next_step'...\n",
            "Decision: End (LLM provided final answer)\n",
            "{'llm_node': {'messages': [AIMessage(content=\"It's 15 degrees Celsius and cloudy in London.\", additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []}, id='run--54416111-f376-4d56-b849-f3991417931b-0', usage_metadata={'input_tokens': 128, 'output_tokens': 13, 'total_tokens': 196, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 55}})]}}\n",
            "\n",
            "--- Final Conversation History (Test 2) ---\n",
            "[AIMessage]: It's 15 degrees Celsius and cloudy in London.\n",
            "\n",
            "\n",
            "===== Running Test Case 3: Query NOT requiring a tool =====\n",
            "\n",
            "--- User Query: 'If I go to London from Dhaka, should I bring my umbrella?' ---\n",
            "Executing 'call_llm_node'...\n",
            "Executing 'decide_next_step'...\n",
            "Decision: Go to 'tool_node' (LLM wants to use a tool)\n",
            "{'llm_node': {'messages': [AIMessage(content='', additional_kwargs={'function_call': {'name': 'get_current_weather', 'arguments': '{\"location\": \"London\"}'}}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []}, id='run--5ab639a2-cb56-4961-8f27-ace4d3373669-0', tool_calls=[{'name': 'get_current_weather', 'args': {'location': 'London'}, 'id': '4f91f896-cb05-42b3-a46b-2c8984a83415', 'type': 'tool_call'}], usage_metadata={'input_tokens': 88, 'output_tokens': 17, 'total_tokens': 198, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 93}})]}}\n",
            "Executing 'execute_tool_node'...\n",
            "Attempting to executing tool :{tool_name}\n",
            "    Tool outputs to add to state: [ToolMessage(content=\"It's 15 degrees Celsius and cloudy in London.\", tool_call_id='4f91f896-cb05-42b3-a46b-2c8984a83415')]\n",
            "{'tool_node': {'messages': [ToolMessage(content=\"It's 15 degrees Celsius and cloudy in London.\", tool_call_id='4f91f896-cb05-42b3-a46b-2c8984a83415')]}}\n",
            "Executing 'call_llm_node'...\n",
            "Executing 'decide_next_step'...\n",
            "Decision: End (LLM provided final answer)\n",
            "{'llm_node': {'messages': [AIMessage(content=\"It's 15 degrees Celsius and cloudy in London. I'd recommend bringing an umbrella.\", additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []}, id='run--445c1d4e-fba8-418b-9235-8f9d3508611f-0', usage_metadata={'input_tokens': 134, 'output_tokens': 21, 'total_tokens': 155, 'input_token_details': {'cache_read': 0}})]}}\n",
            "\n",
            "--- Final Conversation History (Test 3) ---\n",
            "[AIMessage]: It's 15 degrees Celsius and cloudy in London. I'd recommend bringing an umbrella.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Building a Multi-Tool, Multi-Step Agent for Information Retrieval**"
      ],
      "metadata": {
        "id": "b-sPsVPhvTOJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.1 - Enhancing the LLM's Planning with System Messages and Multi-Tool Output"
      ],
      "metadata": {
        "id": "sJpVZYjOvZW_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from langchain_core.tools import tool\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langgraph.graph import StateGraph, END\n",
        "from typing import TypedDict, Annotated\n",
        "import operator\n",
        "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, ToolMessage, SystemMessage\n",
        "from datetime import datetime\n",
        "\n",
        "# Ensure GOOGLE_API_KEY is set up (e.g., from Colab secrets)\n",
        "# from google.colab import userdata\n",
        "# os.environ[\"GOOGLE_API_KEY\"] = userdata.get('GOOGLE_API_KEY')\n",
        "\n",
        "# --- 1. Define Tools (Same as before) ---\n",
        "@tool\n",
        "def get_current_datetime() -> str:\n",
        "    \"\"\"Returns the current date and time.\"\"\"\n",
        "    # Current time is Saturday, June 21, 2025 at 7:38:06 PM +06.\n",
        "    return \"2025-06-21 19:38:06\"\n",
        "\n",
        "@tool\n",
        "def get_current_weather(location: str) -> str:\n",
        "    \"\"\"Gets the current weather for a specified location.\"\"\"\n",
        "    # Current time context: Saturday, June 21, 2025 at 7:38:06 PM +06.\n",
        "    if \"dhaka\" in location.lower():\n",
        "        return \"As of 2025-06-21 19:38:06: It's 30 degrees Celsius and partly cloudy in Dhaka. High humidity.\"\n",
        "    elif \"london\" in location.lower():\n",
        "        return \"As of 2025-06-21 19:38:06: It's 18 degrees Celsius and light rain in London. Bring an umbrella.\"\n",
        "    elif \"paris\" in location.lower():\n",
        "        return \"As of 2025-06-21 19:38:06: It's 22 degrees Celsius and sunny in Paris.\"\n",
        "    else:\n",
        "        return \"Weather data not available for this location.\"\n",
        "\n",
        "tools = [get_current_datetime, get_current_weather]\n",
        "tool_map = {tool.name: tool for tool in tools}\n",
        "\n",
        "# --- 2. Initialize LLM with Tools ---\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\", temperature=0.0) # Still keeping temp low for predictability\n",
        "llm_with_tools = llm.bind_tools(tools)\n",
        "\n",
        "# --- 3. Define Agent State ---\n",
        "class AgentState(TypedDict):\n",
        "    messages: Annotated[list[BaseMessage], operator.add]\n",
        "\n",
        "# --- 4. Define Nodes ---\n",
        "\n",
        "# MODIFIED call_llm_node to include a SystemMessage\n",
        "def call_llm_node(state: AgentState):\n",
        "    print(\"\\n--- Executing 'call_llm_node' ---\")\n",
        "    messages = state[\"messages\"]\n",
        "\n",
        "    # Add a system message at the beginning of the conversation history\n",
        "    # This guides the LLM's behavior throughout the interaction\n",
        "    system_message_content = \"\"\"You are a helpful and meticulous assistant.\n",
        "    When asked about weather for multiple locations, use the 'get_current_weather' tool for EACH location mentioned.\n",
        "    After gathering all necessary information, provide a comprehensive and helpful answer to the user.\n",
        "    If a tool returns information about bringing an umbrella, explicitly mention that in your final answer.\n",
        "    \"\"\"\n",
        "    # Create a new list for the LLM call, starting with SystemMessage\n",
        "    # IMPORTANT: The SystemMessage should be at the very beginning of the messages list\n",
        "    # for the LLM to process it as initial context.\n",
        "    messages_for_llm = [SystemMessage(content=system_message_content)] + messages\n",
        "\n",
        "    response = llm_with_tools.invoke(messages_for_llm)\n",
        "    print(f\"LLM Response (from call_llm_node): {response}\")\n",
        "    return {\"messages\": [response]}\n",
        "\n",
        "# execute_tool_node (No changes needed, your optimized version is perfect for multiple tool calls!)\n",
        "def execute_tool_node(state: AgentState):\n",
        "    print(\"\\n--- Executing 'execute_tool_node' (Optimized) ---\")\n",
        "    messages = state[\"messages\"]\n",
        "    last_message = messages[-1]\n",
        "\n",
        "    if not last_message.tool_calls:\n",
        "        print(\"Warning: execute_tool_node called, but no tool calls found in last message.\")\n",
        "        return {\"messages\": []}\n",
        "\n",
        "    tool_outputs = []\n",
        "    for tool_call in last_message.tool_calls:\n",
        "        tool_name = tool_call['name']\n",
        "        tool_args = tool_call['args']\n",
        "        tool_call_id = tool_call['id']\n",
        "\n",
        "        print(f\"  Attempting to execute tool: '{tool_name}' with arguments: {tool_args}\")\n",
        "\n",
        "        if tool_name in tool_map:\n",
        "            called_tool = tool_map[tool_name]\n",
        "            try:\n",
        "                output = called_tool.invoke(tool_args)\n",
        "                tool_outputs.append(ToolMessage(content=output, tool_call_id=tool_call_id))\n",
        "            except Exception as e:\n",
        "                error_message = f\"Error executing tool '{tool_name}': {e}\"\n",
        "                print(error_message)\n",
        "                tool_outputs.append(ToolMessage(content=error_message, tool_call_id=tool_call_id))\n",
        "        else:\n",
        "            error_message = f\"Error: Unknown tool called by LLM: '{tool_name}'\"\n",
        "            print(error_message)\n",
        "            tool_outputs.append(ToolMessage(content=error_message, tool_call_id=tool_call_id))\n",
        "\n",
        "    print(f\"  Tool outputs to add to state: {tool_outputs}\")\n",
        "    return {\"messages\": tool_outputs}\n",
        "\n",
        "# --- 5. Define Conditional Edge Logic (Same as before) ---\n",
        "def decide_next_step(state: AgentState):\n",
        "    last_message = state[\"messages\"][-1]\n",
        "    if last_message.tool_calls:\n",
        "        print(\"\\n--- Decision: LLM wants to use a tool. Routing to 'tool_node'. ---\")\n",
        "        return \"tool\"\n",
        "    else:\n",
        "        print(\"\\n--- Decision: LLM provided final answer. Routing to 'END'. ---\")\n",
        "        return \"end\"\n",
        "\n",
        "# --- 6. Build the LangGraph Workflow (Same as before) ---\n",
        "workflow = StateGraph(AgentState)\n",
        "workflow.add_node(\"llm_node\", call_llm_node)\n",
        "workflow.add_node(\"tool_node\", execute_tool_node)\n",
        "workflow.set_entry_point(\"llm_node\")\n",
        "workflow.add_conditional_edges(\n",
        "    \"llm_node\",\n",
        "    decide_next_step,\n",
        "    {\n",
        "        \"tool\": \"tool_node\",\n",
        "        \"end\": END\n",
        "    }\n",
        ")\n",
        "workflow.add_edge(\"tool_node\", \"llm_node\")\n",
        "\n",
        "# --- 7. Compile the Graph ---\n",
        "app = workflow.compile()\n",
        "\n",
        "# --- 8. Test Case for Multi-Location Query ---\n",
        "\n",
        "print(\"\\n===== Running Test Case: Multi-Location Weather Query =====\")\n",
        "user_query_multi_location = \"If I go to London from Dhaka, should I bring my umbrella?\"\n",
        "initial_state_multi_location = {\"messages\": [HumanMessage(content=user_query_multi_location)]}\n",
        "\n",
        "print(f\"\\n--- User Query: '{user_query_multi_location}' ---\")\n",
        "final_state_multi_location = {}\n",
        "for s in app.stream(initial_state_multi_location):\n",
        "    print(s)\n",
        "    if list(s.values()) and 'messages' in list(s.values())[0]:\n",
        "        final_state_multi_location = list(s.values())[0]\n",
        "\n",
        "print(\"\\n--- Final Conversation History (Multi-Location Test) ---\")\n",
        "if final_state_multi_location:\n",
        "    for msg in final_state_multi_location['messages']:\n",
        "        print(f\"[{msg.__class__.__name__}]: {msg.content.strip()}\")\n",
        "        if hasattr(msg, 'tool_calls') and msg.tool_calls:\n",
        "            print(f\"  Tool Calls: {msg.tool_calls}\")\n",
        "        if hasattr(msg, 'tool_call_id') and msg.tool_call_id:\n",
        "            print(f\"  Tool Call ID: {msg.tool_call_id}\")\n",
        "else:\n",
        "    print(\"No final state captured for Multi-Location Test.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Y1WxYSMvdpX",
        "outputId": "583eb179-a848-4e7a-cc6a-0ab34039b541"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== Running Test Case: Multi-Location Weather Query =====\n",
            "\n",
            "--- User Query: 'If I go to London from Dhaka, should I bring my umbrella?' ---\n",
            "\n",
            "--- Executing 'call_llm_node' ---\n",
            "LLM Response (from call_llm_node): content='' additional_kwargs={'function_call': {'name': 'get_current_weather', 'arguments': '{\"location\": \"London\"}'}} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []} id='run--2fec097e-6a4e-481b-a13a-754801969460-0' tool_calls=[{'name': 'get_current_weather', 'args': {'location': 'London'}, 'id': 'f0028de8-cfe2-46b7-b423-a610df8d06de', 'type': 'tool_call'}] usage_metadata={'input_tokens': 161, 'output_tokens': 17, 'total_tokens': 236, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 58}}\n",
            "\n",
            "--- Decision: LLM wants to use a tool. Routing to 'tool_node'. ---\n",
            "{'llm_node': {'messages': [AIMessage(content='', additional_kwargs={'function_call': {'name': 'get_current_weather', 'arguments': '{\"location\": \"London\"}'}}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []}, id='run--2fec097e-6a4e-481b-a13a-754801969460-0', tool_calls=[{'name': 'get_current_weather', 'args': {'location': 'London'}, 'id': 'f0028de8-cfe2-46b7-b423-a610df8d06de', 'type': 'tool_call'}], usage_metadata={'input_tokens': 161, 'output_tokens': 17, 'total_tokens': 236, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 58}})]}}\n",
            "\n",
            "--- Executing 'execute_tool_node' (Optimized) ---\n",
            "  Attempting to execute tool: 'get_current_weather' with arguments: {'location': 'London'}\n",
            "  Tool outputs to add to state: [ToolMessage(content=\"As of 2025-06-21 19:38:06: It's 18 degrees Celsius and light rain in London. Bring an umbrella.\", tool_call_id='f0028de8-cfe2-46b7-b423-a610df8d06de')]\n",
            "{'tool_node': {'messages': [ToolMessage(content=\"As of 2025-06-21 19:38:06: It's 18 degrees Celsius and light rain in London. Bring an umbrella.\", tool_call_id='f0028de8-cfe2-46b7-b423-a610df8d06de')]}}\n",
            "\n",
            "--- Executing 'call_llm_node' ---\n",
            "LLM Response (from call_llm_node): content=\"Yes, you should bring an umbrella for your trip to London. It's currently 18 degrees Celsius with light rain there.\" additional_kwargs={} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []} id='run--e114b7b1-b925-4d72-a170-6b9d5a34b69e-0' usage_metadata={'input_tokens': 235, 'output_tokens': 27, 'total_tokens': 331, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 69}}\n",
            "\n",
            "--- Decision: LLM provided final answer. Routing to 'END'. ---\n",
            "{'llm_node': {'messages': [AIMessage(content=\"Yes, you should bring an umbrella for your trip to London. It's currently 18 degrees Celsius with light rain there.\", additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []}, id='run--e114b7b1-b925-4d72-a170-6b9d5a34b69e-0', usage_metadata={'input_tokens': 235, 'output_tokens': 27, 'total_tokens': 331, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 69}})]}}\n",
            "\n",
            "--- Final Conversation History (Multi-Location Test) ---\n",
            "[AIMessage]: Yes, you should bring an umbrella for your trip to London. It's currently 18 degrees Celsius with light rain there.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.2: Implementing a Basic ReAct-like Agent"
      ],
      "metadata": {
        "id": "_o4z-1KAwUpA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from langchain_core.tools import tool\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langgraph.graph import StateGraph, END\n",
        "from typing import TypedDict, Annotated\n",
        "import operator\n",
        "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, ToolMessage, SystemMessage\n",
        "from datetime import datetime\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "\n",
        "# Ensure GOOGLE_API_KEY is set up (e.g., from Colab secrets)\n",
        "# from google.colab import userdata\n",
        "# os.environ[\"GOOGLE_API_KEY\"] = userdata.get('GOOGLE_API_KEY')\n",
        "\n",
        "# --- 1. Define Tools (Same as before) ---\n",
        "@tool\n",
        "def get_current_datetime() -> str:\n",
        "    \"\"\"Returns the current date and time.\"\"\"\n",
        "    return \"2025-06-21 19:38:06\" # Keeping consistent for reproducibility\n",
        "\n",
        "@tool\n",
        "def get_current_weather(location: str) -> str:\n",
        "    \"\"\"Gets the current weather for a specified location.\n",
        "    If the location is not available, state that weather data is not available.\n",
        "    \"\"\"\n",
        "    if \"dhaka\" in location.lower():\n",
        "        return \"As of 2025-06-21 19:38:06: It's 30 degrees Celsius and partly cloudy in Dhaka. High humidity.\"\n",
        "    elif \"london\" in location.lower():\n",
        "        return \"As of 2025-06-21 19:38:06: It's 18 degrees Celsius and light rain in London. Bring an umbrella.\"\n",
        "    elif \"paris\" in location.lower():\n",
        "        return \"As of 2025-06-21 19:38:06: It's 22 degrees Celsius and sunny in Paris.\"\n",
        "    else:\n",
        "        return \"Weather data not available for this location.\"\n",
        "\n",
        "tools = [get_current_datetime, get_current_weather]\n",
        "tool_map = {tool.name: tool for tool in tools}\n",
        "\n",
        "# --- 2. Initialize LLM with Tools ---\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\", temperature=0.0) # Still keeping temp low for predictability\n",
        "\n",
        "# IMPORTANT: For ReAct, we'll build a custom prompt that includes tool descriptions\n",
        "# and guides the LLM to output \"Thought\" and \"Action\"\n",
        "# We will bind tools to this *chain*, not directly to the LLM\n",
        "# The LLM itself will not have tools bound initially; the prompt will describe them.\n",
        "\n",
        "# --- 3. Define Agent State (Same) ---\n",
        "class AgentState(TypedDict):\n",
        "    messages: Annotated[list[BaseMessage], operator.add]\n",
        "\n",
        "# --- 4. Define Nodes ---\n",
        "\n",
        "# MODIFIED call_llm_node for ReAct\n",
        "def call_llm_node(state: AgentState):\n",
        "    print(\"\\n--- Executing 'call_llm_node' (ReAct Mode) ---\")\n",
        "    messages = state[\"messages\"]\n",
        "\n",
        "    # This prompt tells the LLM to think step-by-step (Thought)\n",
        "    # and then decide on an Action (using a tool) or a Final Answer.\n",
        "    # It dynamically includes the tool descriptions.\n",
        "    prompt = ChatPromptTemplate.from_messages(\n",
        "        [\n",
        "            (\n",
        "                \"system\",\n",
        "                \"You are a helpful AI assistant. You have access to the following tools:\\n\"\n",
        "                f\"{tools}\\n\\n\" # This injects tool descriptions into the prompt\n",
        "                \"You must use the provided tools to answer questions when appropriate.\\n\"\n",
        "                \"If you need to use a tool, first output your 'Thought' about why you need it, \"\n",
        "                \"then output the 'Action' (a tool call). You can make multiple tool calls if needed \"\n",
        "                \"to gather all necessary information.\\n\"\n",
        "                \"If you have gathered all necessary information and can answer the user's query, \"\n",
        "                \"output your 'Thought' about the answer, then output a 'Final Answer'.\\n\\n\"\n",
        "                \"Current Conversation History (including tool observations):\"\n",
        "            ),\n",
        "            (\"placeholder\", \"{messages}\") # This will be replaced by the actual message history\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    # Create a runnable that pipes messages through the prompt and then to the LLM\n",
        "    # We use .bind_tools() on the LLM itself, which uses the model's native function calling\n",
        "    # The prompt helps guide the *structure* of the LLM's response (Thought/Action)\n",
        "    # while bind_tools handles the *format* of the tool call JSON.\n",
        "    llm_chain = prompt | llm.bind_tools(tools)\n",
        "    response = llm_chain.invoke({\"messages\": messages})\n",
        "\n",
        "    print(f\"LLM Response (from call_llm_node): {response}\")\n",
        "    return {\"messages\": [response]}\n",
        "\n",
        "# execute_tool_node (No changes needed, still perfect for multiple tool calls!)\n",
        "def execute_tool_node(state: AgentState):\n",
        "    print(\"\\n--- Executing 'execute_tool_node' (Optimized) ---\")\n",
        "    messages = state[\"messages\"]\n",
        "    last_message = messages[-1]\n",
        "\n",
        "    if not last_message.tool_calls:\n",
        "        print(\"Warning: execute_tool_node called, but no tool calls found in last message.\")\n",
        "        return {\"messages\": []}\n",
        "\n",
        "    tool_outputs = []\n",
        "    for tool_call in last_message.tool_calls:\n",
        "        tool_name = tool_call['name']\n",
        "        tool_args = tool_call['args']\n",
        "        tool_call_id = tool_call['id']\n",
        "\n",
        "        print(f\"  Attempting to execute tool: '{tool_name}' with arguments: {tool_args}\")\n",
        "\n",
        "        if tool_name in tool_map:\n",
        "            called_tool = tool_map[tool_name]\n",
        "            try:\n",
        "                output = called_tool.invoke(tool_args)\n",
        "                tool_outputs.append(ToolMessage(content=output, tool_call_id=tool_call_id))\n",
        "            except Exception as e:\n",
        "                error_message = f\"Error executing tool '{tool_name}': {e}\"\n",
        "                print(error_message)\n",
        "                tool_outputs.append(ToolMessage(content=error_message, tool_call_id=tool_call_id))\n",
        "        else:\n",
        "            error_message = f\"Error: Unknown tool called by LLM: '{tool_name}'\"\n",
        "            print(error_message)\n",
        "            tool_outputs.append(ToolMessage(content=error_message, tool_call_id=tool_call_id))\n",
        "\n",
        "    print(f\"  Tool outputs to add to state: {tool_outputs}\")\n",
        "    return {\"messages\": tool_outputs}\n",
        "\n",
        "# MODIFIED decide_next_step for ReAct\n",
        "def decide_next_step(state: AgentState):\n",
        "    last_message = state[\"messages\"][-1]\n",
        "\n",
        "    # Check if the LLM provided a final answer (look for \"Final Answer\" in its content)\n",
        "    # This is a heuristic, better parsing might be needed for production\n",
        "    if \"final answer\" in last_message.content.lower():\n",
        "        print(\"\\n--- Decision: LLM provided Final Answer. Routing to 'END'. ---\")\n",
        "        return \"end\"\n",
        "\n",
        "    # If not a final answer, it must be a tool call (or an error, which will loop back to LLM eventually)\n",
        "    elif last_message.tool_calls:\n",
        "        print(\"\\n--- Decision: LLM wants to use a tool. Routing to 'tool_node'. ---\")\n",
        "        return \"tool\"\n",
        "    else:\n",
        "        # Fallback for unexpected LLM output (e.g., just text but not a final answer)\n",
        "        # This will send it back to the LLM to try again, perhaps with more context.\n",
        "        print(\"\\n--- Decision: Unexpected LLM output, looping back to LLM. ---\")\n",
        "        return \"llm_node\" # Loop back to LLM if it didn't use a tool or give a final answer\n",
        "\n",
        "# --- 6. Build the LangGraph Workflow (Same Graph Structure) ---\n",
        "workflow = StateGraph(AgentState)\n",
        "workflow.add_node(\"llm_node\", call_llm_node)\n",
        "workflow.add_node(\"tool_node\", execute_tool_node)\n",
        "workflow.set_entry_point(\"llm_node\")\n",
        "\n",
        "workflow.add_conditional_edges(\n",
        "    \"llm_node\",\n",
        "    decide_next_step,\n",
        "    {\n",
        "        \"tool\": \"tool_node\",\n",
        "        \"end\": END,\n",
        "        \"llm_node\": \"llm_node\" # Self-loop for unexpected output\n",
        "    }\n",
        ")\n",
        "workflow.add_edge(\"tool_node\", \"llm_node\") # Always loop back to LLM after tool execution\n",
        "\n",
        "# --- 7. Compile the Graph ---\n",
        "app = workflow.compile()\n",
        "\n",
        "# --- 8. Test Case for Multi-Location Query ---\n",
        "\n",
        "print(\"\\n===== Running Test Case: Multi-Location Weather Query (ReAct) =====\")\n",
        "user_query_multi_location = \"If I go to London from Dhaka, should I bring my umbrella?\"\n",
        "initial_state_multi_location = {\"messages\": [HumanMessage(content=user_query_multi_location)]}\n",
        "\n",
        "print(f\"\\n--- User Query: '{user_query_multi_location}' ---\")\n",
        "final_state_multi_location = {}\n",
        "for s in app.stream(initial_state_multi_location):\n",
        "    print(s)\n",
        "    if list(s.values()) and 'messages' in list(s.values())[0]:\n",
        "        final_state_multi_location = list(s.values())[0]\n",
        "\n",
        "print(\"\\n--- Final Conversation History (Multi-Location Test) ---\")\n",
        "if final_state_multi_location:\n",
        "    for msg in final_state_multi_location['messages']:\n",
        "        print(f\"[{msg.__class__.__name__}]: {msg.content.strip()}\")\n",
        "        if hasattr(msg, 'tool_calls') and msg.tool_calls:\n",
        "            print(f\"  Tool Calls: {msg.tool_calls}\")\n",
        "        if hasattr(msg, 'tool_call_id') and msg.tool_call_id:\n",
        "            print(f\"  Tool Call ID: {msg.tool_call_id}\")\n",
        "else:\n",
        "    print(\"No final state captured for Multi-Location Test.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "QEOQhKO5wi23",
        "outputId": "5fd8b953-df4f-4f1b-8743-2c5956dd6633"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== Running Test Case: Multi-Location Weather Query (ReAct) =====\n",
            "\n",
            "--- User Query: 'If I go to London from Dhaka, should I bring my umbrella?' ---\n",
            "\n",
            "--- Executing 'call_llm_node' (ReAct Mode) ---\n",
            "LLM Response (from call_llm_node): content='' additional_kwargs={'function_call': {'name': 'get_current_weather', 'arguments': '{\"location\": \"London\"}'}} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []} id='run--d4f17b0a-b885-49a9-865a-23a96c481172-0' tool_calls=[{'name': 'get_current_weather', 'args': {'location': 'London'}, 'id': '8a8f60af-ad5b-4c2b-8f59-7eee44bd1add', 'type': 'tool_call'}] usage_metadata={'input_tokens': 379, 'output_tokens': 17, 'total_tokens': 439, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 43}}\n",
            "\n",
            "--- Decision: LLM wants to use a tool. Routing to 'tool_node'. ---\n",
            "{'llm_node': {'messages': [AIMessage(content='', additional_kwargs={'function_call': {'name': 'get_current_weather', 'arguments': '{\"location\": \"London\"}'}}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []}, id='run--d4f17b0a-b885-49a9-865a-23a96c481172-0', tool_calls=[{'name': 'get_current_weather', 'args': {'location': 'London'}, 'id': '8a8f60af-ad5b-4c2b-8f59-7eee44bd1add', 'type': 'tool_call'}], usage_metadata={'input_tokens': 379, 'output_tokens': 17, 'total_tokens': 439, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 43}})]}}\n",
            "\n",
            "--- Executing 'execute_tool_node' (Optimized) ---\n",
            "  Attempting to execute tool: 'get_current_weather' with arguments: {'location': 'London'}\n",
            "  Tool outputs to add to state: [ToolMessage(content=\"As of 2025-06-21 19:38:06: It's 18 degrees Celsius and light rain in London. Bring an umbrella.\", tool_call_id='8a8f60af-ad5b-4c2b-8f59-7eee44bd1add')]\n",
            "{'tool_node': {'messages': [ToolMessage(content=\"As of 2025-06-21 19:38:06: It's 18 degrees Celsius and light rain in London. Bring an umbrella.\", tool_call_id='8a8f60af-ad5b-4c2b-8f59-7eee44bd1add')]}}\n",
            "\n",
            "--- Executing 'call_llm_node' (ReAct Mode) ---\n",
            "LLM Response (from call_llm_node): content=\"It's currently 18 degrees Celsius with light rain in London, so yes, you should bring your umbrella.\" additional_kwargs={} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []} id='run--c75b2215-6c37-4def-afdc-ea2727413d12-0' usage_metadata={'input_tokens': 453, 'output_tokens': 24, 'total_tokens': 537, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 60}}\n",
            "\n",
            "--- Decision: Unexpected LLM output, looping back to LLM. ---\n",
            "{'llm_node': {'messages': [AIMessage(content=\"It's currently 18 degrees Celsius with light rain in London, so yes, you should bring your umbrella.\", additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []}, id='run--c75b2215-6c37-4def-afdc-ea2727413d12-0', usage_metadata={'input_tokens': 453, 'output_tokens': 24, 'total_tokens': 537, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 60}})]}}\n",
            "\n",
            "--- Executing 'call_llm_node' (ReAct Mode) ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_google_genai.chat_models:Gemini produced an empty response. Continuing with empty message\n",
            "Feedback: \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LLM Response (from call_llm_node): content='' additional_kwargs={} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}} id='run--76076e78-8eb1-4d47-9cb9-1265cf54187a-0'\n",
            "\n",
            "--- Decision: Unexpected LLM output, looping back to LLM. ---\n",
            "{'llm_node': {'messages': [AIMessage(content='', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}}, id='run--76076e78-8eb1-4d47-9cb9-1265cf54187a-0')]}}\n",
            "\n",
            "--- Executing 'call_llm_node' (ReAct Mode) ---\n",
            "LLM Response (from call_llm_node): content='' additional_kwargs={'function_call': {'name': 'get_current_weather', 'arguments': '{\"location\": \"London\"}'}} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []} id='run--5f0680e9-8e9b-4b4c-b927-176065f0284e-0' tool_calls=[{'name': 'get_current_weather', 'args': {'location': 'London'}, 'id': 'e68c4e34-d737-429a-a1ef-9fec29b35631', 'type': 'tool_call'}] usage_metadata={'input_tokens': 479, 'output_tokens': 17, 'total_tokens': 590, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 94}}\n",
            "\n",
            "--- Decision: LLM wants to use a tool. Routing to 'tool_node'. ---\n",
            "{'llm_node': {'messages': [AIMessage(content='', additional_kwargs={'function_call': {'name': 'get_current_weather', 'arguments': '{\"location\": \"London\"}'}}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []}, id='run--5f0680e9-8e9b-4b4c-b927-176065f0284e-0', tool_calls=[{'name': 'get_current_weather', 'args': {'location': 'London'}, 'id': 'e68c4e34-d737-429a-a1ef-9fec29b35631', 'type': 'tool_call'}], usage_metadata={'input_tokens': 479, 'output_tokens': 17, 'total_tokens': 590, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 94}})]}}\n",
            "\n",
            "--- Executing 'execute_tool_node' (Optimized) ---\n",
            "  Attempting to execute tool: 'get_current_weather' with arguments: {'location': 'London'}\n",
            "  Tool outputs to add to state: [ToolMessage(content=\"As of 2025-06-21 19:38:06: It's 18 degrees Celsius and light rain in London. Bring an umbrella.\", tool_call_id='e68c4e34-d737-429a-a1ef-9fec29b35631')]\n",
            "{'tool_node': {'messages': [ToolMessage(content=\"As of 2025-06-21 19:38:06: It's 18 degrees Celsius and light rain in London. Bring an umbrella.\", tool_call_id='e68c4e34-d737-429a-a1ef-9fec29b35631')]}}\n",
            "\n",
            "--- Executing 'call_llm_node' (ReAct Mode) ---\n",
            "LLM Response (from call_llm_node): content='' additional_kwargs={'function_call': {'name': 'get_current_weather', 'arguments': '{\"location\": \"London\"}'}} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []} id='run--8be9c4c8-cea2-4b09-8c0a-516b6d43bbe6-0' tool_calls=[{'name': 'get_current_weather', 'args': {'location': 'London'}, 'id': '35ff5db6-915e-467d-af81-c0c1c98b870f', 'type': 'tool_call'}] usage_metadata={'input_tokens': 553, 'output_tokens': 17, 'total_tokens': 627, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 57}}\n",
            "\n",
            "--- Decision: LLM wants to use a tool. Routing to 'tool_node'. ---\n",
            "{'llm_node': {'messages': [AIMessage(content='', additional_kwargs={'function_call': {'name': 'get_current_weather', 'arguments': '{\"location\": \"London\"}'}}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []}, id='run--8be9c4c8-cea2-4b09-8c0a-516b6d43bbe6-0', tool_calls=[{'name': 'get_current_weather', 'args': {'location': 'London'}, 'id': '35ff5db6-915e-467d-af81-c0c1c98b870f', 'type': 'tool_call'}], usage_metadata={'input_tokens': 553, 'output_tokens': 17, 'total_tokens': 627, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 57}})]}}\n",
            "\n",
            "--- Executing 'execute_tool_node' (Optimized) ---\n",
            "  Attempting to execute tool: 'get_current_weather' with arguments: {'location': 'London'}\n",
            "  Tool outputs to add to state: [ToolMessage(content=\"As of 2025-06-21 19:38:06: It's 18 degrees Celsius and light rain in London. Bring an umbrella.\", tool_call_id='35ff5db6-915e-467d-af81-c0c1c98b870f')]\n",
            "{'tool_node': {'messages': [ToolMessage(content=\"As of 2025-06-21 19:38:06: It's 18 degrees Celsius and light rain in London. Bring an umbrella.\", tool_call_id='35ff5db6-915e-467d-af81-c0c1c98b870f')]}}\n",
            "\n",
            "--- Executing 'call_llm_node' (ReAct Mode) ---\n",
            "LLM Response (from call_llm_node): content='' additional_kwargs={'function_call': {'name': 'get_current_weather', 'arguments': '{\"location\": \"London\"}'}} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []} id='run--20d219ee-ba3e-40d2-a029-add27f528314-0' tool_calls=[{'name': 'get_current_weather', 'args': {'location': 'London'}, 'id': '78e14d42-8661-45a5-b771-db8ff4ce910a', 'type': 'tool_call'}] usage_metadata={'input_tokens': 627, 'output_tokens': 17, 'total_tokens': 693, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 49}}\n",
            "\n",
            "--- Decision: LLM wants to use a tool. Routing to 'tool_node'. ---\n",
            "{'llm_node': {'messages': [AIMessage(content='', additional_kwargs={'function_call': {'name': 'get_current_weather', 'arguments': '{\"location\": \"London\"}'}}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []}, id='run--20d219ee-ba3e-40d2-a029-add27f528314-0', tool_calls=[{'name': 'get_current_weather', 'args': {'location': 'London'}, 'id': '78e14d42-8661-45a5-b771-db8ff4ce910a', 'type': 'tool_call'}], usage_metadata={'input_tokens': 627, 'output_tokens': 17, 'total_tokens': 693, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 49}})]}}\n",
            "\n",
            "--- Executing 'execute_tool_node' (Optimized) ---\n",
            "  Attempting to execute tool: 'get_current_weather' with arguments: {'location': 'London'}\n",
            "  Tool outputs to add to state: [ToolMessage(content=\"As of 2025-06-21 19:38:06: It's 18 degrees Celsius and light rain in London. Bring an umbrella.\", tool_call_id='78e14d42-8661-45a5-b771-db8ff4ce910a')]\n",
            "{'tool_node': {'messages': [ToolMessage(content=\"As of 2025-06-21 19:38:06: It's 18 degrees Celsius and light rain in London. Bring an umbrella.\", tool_call_id='78e14d42-8661-45a5-b771-db8ff4ce910a')]}}\n",
            "\n",
            "--- Executing 'call_llm_node' (ReAct Mode) ---\n",
            "LLM Response (from call_llm_node): content='' additional_kwargs={'function_call': {'name': 'get_current_weather', 'arguments': '{\"location\": \"London\"}'}} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []} id='run--b2e00c98-7779-4177-b288-4d49112fd20f-0' tool_calls=[{'name': 'get_current_weather', 'args': {'location': 'London'}, 'id': 'c8f2fcb0-9ec5-4339-80f0-0411c28f6a84', 'type': 'tool_call'}] usage_metadata={'input_tokens': 701, 'output_tokens': 17, 'total_tokens': 775, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 57}}\n",
            "\n",
            "--- Decision: LLM wants to use a tool. Routing to 'tool_node'. ---\n",
            "{'llm_node': {'messages': [AIMessage(content='', additional_kwargs={'function_call': {'name': 'get_current_weather', 'arguments': '{\"location\": \"London\"}'}}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []}, id='run--b2e00c98-7779-4177-b288-4d49112fd20f-0', tool_calls=[{'name': 'get_current_weather', 'args': {'location': 'London'}, 'id': 'c8f2fcb0-9ec5-4339-80f0-0411c28f6a84', 'type': 'tool_call'}], usage_metadata={'input_tokens': 701, 'output_tokens': 17, 'total_tokens': 775, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 57}})]}}\n",
            "\n",
            "--- Executing 'execute_tool_node' (Optimized) ---\n",
            "  Attempting to execute tool: 'get_current_weather' with arguments: {'location': 'London'}\n",
            "  Tool outputs to add to state: [ToolMessage(content=\"As of 2025-06-21 19:38:06: It's 18 degrees Celsius and light rain in London. Bring an umbrella.\", tool_call_id='c8f2fcb0-9ec5-4339-80f0-0411c28f6a84')]\n",
            "{'tool_node': {'messages': [ToolMessage(content=\"As of 2025-06-21 19:38:06: It's 18 degrees Celsius and light rain in London. Bring an umbrella.\", tool_call_id='c8f2fcb0-9ec5-4339-80f0-0411c28f6a84')]}}\n",
            "\n",
            "--- Executing 'call_llm_node' (ReAct Mode) ---\n",
            "LLM Response (from call_llm_node): content='' additional_kwargs={'function_call': {'name': 'get_current_weather', 'arguments': '{\"location\": \"London\"}'}} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []} id='run--65c002a8-75cd-4f82-b923-e7c4e2ba60b5-0' tool_calls=[{'name': 'get_current_weather', 'args': {'location': 'London'}, 'id': 'f5f2ee29-0320-4b7a-8403-5d1c4077d031', 'type': 'tool_call'}] usage_metadata={'input_tokens': 775, 'output_tokens': 17, 'total_tokens': 849, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 57}}\n",
            "\n",
            "--- Decision: LLM wants to use a tool. Routing to 'tool_node'. ---\n",
            "{'llm_node': {'messages': [AIMessage(content='', additional_kwargs={'function_call': {'name': 'get_current_weather', 'arguments': '{\"location\": \"London\"}'}}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []}, id='run--65c002a8-75cd-4f82-b923-e7c4e2ba60b5-0', tool_calls=[{'name': 'get_current_weather', 'args': {'location': 'London'}, 'id': 'f5f2ee29-0320-4b7a-8403-5d1c4077d031', 'type': 'tool_call'}], usage_metadata={'input_tokens': 775, 'output_tokens': 17, 'total_tokens': 849, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 57}})]}}\n",
            "\n",
            "--- Executing 'execute_tool_node' (Optimized) ---\n",
            "  Attempting to execute tool: 'get_current_weather' with arguments: {'location': 'London'}\n",
            "  Tool outputs to add to state: [ToolMessage(content=\"As of 2025-06-21 19:38:06: It's 18 degrees Celsius and light rain in London. Bring an umbrella.\", tool_call_id='f5f2ee29-0320-4b7a-8403-5d1c4077d031')]\n",
            "{'tool_node': {'messages': [ToolMessage(content=\"As of 2025-06-21 19:38:06: It's 18 degrees Celsius and light rain in London. Bring an umbrella.\", tool_call_id='f5f2ee29-0320-4b7a-8403-5d1c4077d031')]}}\n",
            "\n",
            "--- Executing 'call_llm_node' (ReAct Mode) ---\n",
            "LLM Response (from call_llm_node): content='' additional_kwargs={'function_call': {'name': 'get_current_weather', 'arguments': '{\"location\": \"London\"}'}} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []} id='run--2c051e9d-b06f-4971-8ca7-af8433698fbc-0' tool_calls=[{'name': 'get_current_weather', 'args': {'location': 'London'}, 'id': 'ddef3305-2730-4c31-b0c3-322265820e8d', 'type': 'tool_call'}] usage_metadata={'input_tokens': 849, 'output_tokens': 17, 'total_tokens': 923, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 57}}\n",
            "\n",
            "--- Decision: LLM wants to use a tool. Routing to 'tool_node'. ---\n",
            "{'llm_node': {'messages': [AIMessage(content='', additional_kwargs={'function_call': {'name': 'get_current_weather', 'arguments': '{\"location\": \"London\"}'}}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []}, id='run--2c051e9d-b06f-4971-8ca7-af8433698fbc-0', tool_calls=[{'name': 'get_current_weather', 'args': {'location': 'London'}, 'id': 'ddef3305-2730-4c31-b0c3-322265820e8d', 'type': 'tool_call'}], usage_metadata={'input_tokens': 849, 'output_tokens': 17, 'total_tokens': 923, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 57}})]}}\n",
            "\n",
            "--- Executing 'execute_tool_node' (Optimized) ---\n",
            "  Attempting to execute tool: 'get_current_weather' with arguments: {'location': 'London'}\n",
            "  Tool outputs to add to state: [ToolMessage(content=\"As of 2025-06-21 19:38:06: It's 18 degrees Celsius and light rain in London. Bring an umbrella.\", tool_call_id='ddef3305-2730-4c31-b0c3-322265820e8d')]\n",
            "{'tool_node': {'messages': [ToolMessage(content=\"As of 2025-06-21 19:38:06: It's 18 degrees Celsius and light rain in London. Bring an umbrella.\", tool_call_id='ddef3305-2730-4c31-b0c3-322265820e8d')]}}\n",
            "\n",
            "--- Executing 'call_llm_node' (ReAct Mode) ---\n",
            "LLM Response (from call_llm_node): content='' additional_kwargs={'function_call': {'name': 'get_current_weather', 'arguments': '{\"location\": \"London\"}'}} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []} id='run--b3e34dd0-ea15-4d86-8df7-3b0c722a3ed2-0' tool_calls=[{'name': 'get_current_weather', 'args': {'location': 'London'}, 'id': 'c50f3719-9dd3-4c1a-9589-6fd6f0357296', 'type': 'tool_call'}] usage_metadata={'input_tokens': 923, 'output_tokens': 17, 'total_tokens': 999, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 59}}\n",
            "\n",
            "--- Decision: LLM wants to use a tool. Routing to 'tool_node'. ---\n",
            "{'llm_node': {'messages': [AIMessage(content='', additional_kwargs={'function_call': {'name': 'get_current_weather', 'arguments': '{\"location\": \"London\"}'}}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []}, id='run--b3e34dd0-ea15-4d86-8df7-3b0c722a3ed2-0', tool_calls=[{'name': 'get_current_weather', 'args': {'location': 'London'}, 'id': 'c50f3719-9dd3-4c1a-9589-6fd6f0357296', 'type': 'tool_call'}], usage_metadata={'input_tokens': 923, 'output_tokens': 17, 'total_tokens': 999, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 59}})]}}\n",
            "\n",
            "--- Executing 'execute_tool_node' (Optimized) ---\n",
            "  Attempting to execute tool: 'get_current_weather' with arguments: {'location': 'London'}\n",
            "  Tool outputs to add to state: [ToolMessage(content=\"As of 2025-06-21 19:38:06: It's 18 degrees Celsius and light rain in London. Bring an umbrella.\", tool_call_id='c50f3719-9dd3-4c1a-9589-6fd6f0357296')]\n",
            "{'tool_node': {'messages': [ToolMessage(content=\"As of 2025-06-21 19:38:06: It's 18 degrees Celsius and light rain in London. Bring an umbrella.\", tool_call_id='c50f3719-9dd3-4c1a-9589-6fd6f0357296')]}}\n",
            "\n",
            "--- Executing 'call_llm_node' (ReAct Mode) ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_google_genai.chat_models:Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n",
            "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
            "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
            "  quota_dimensions {\n",
            "    key: \"model\"\n",
            "    value: \"gemini-2.5-flash\"\n",
            "  }\n",
            "  quota_dimensions {\n",
            "    key: \"location\"\n",
            "    value: \"global\"\n",
            "  }\n",
            "  quota_value: 10\n",
            "}\n",
            ", links {\n",
            "  description: \"Learn more about Gemini API quotas\"\n",
            "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "}\n",
            ", retry_delay {\n",
            "  seconds: 27\n",
            "}\n",
            "].\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LLM Response (from call_llm_node): content='' additional_kwargs={'function_call': {'name': 'get_current_weather', 'arguments': '{\"location\": \"London\"}'}} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []} id='run--9144c0ed-ff3c-4d06-a890-b4b55acdd6cd-0' tool_calls=[{'name': 'get_current_weather', 'args': {'location': 'London'}, 'id': '94d0a8d8-bf2e-4eb1-b2d2-29606cc6549a', 'type': 'tool_call'}] usage_metadata={'input_tokens': 997, 'output_tokens': 17, 'total_tokens': 1073, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 59}}\n",
            "\n",
            "--- Decision: LLM wants to use a tool. Routing to 'tool_node'. ---\n",
            "{'llm_node': {'messages': [AIMessage(content='', additional_kwargs={'function_call': {'name': 'get_current_weather', 'arguments': '{\"location\": \"London\"}'}}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []}, id='run--9144c0ed-ff3c-4d06-a890-b4b55acdd6cd-0', tool_calls=[{'name': 'get_current_weather', 'args': {'location': 'London'}, 'id': '94d0a8d8-bf2e-4eb1-b2d2-29606cc6549a', 'type': 'tool_call'}], usage_metadata={'input_tokens': 997, 'output_tokens': 17, 'total_tokens': 1073, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 59}})]}}\n",
            "\n",
            "--- Executing 'execute_tool_node' (Optimized) ---\n",
            "  Attempting to execute tool: 'get_current_weather' with arguments: {'location': 'London'}\n",
            "  Tool outputs to add to state: [ToolMessage(content=\"As of 2025-06-21 19:38:06: It's 18 degrees Celsius and light rain in London. Bring an umbrella.\", tool_call_id='94d0a8d8-bf2e-4eb1-b2d2-29606cc6549a')]\n",
            "{'tool_node': {'messages': [ToolMessage(content=\"As of 2025-06-21 19:38:06: It's 18 degrees Celsius and light rain in London. Bring an umbrella.\", tool_call_id='94d0a8d8-bf2e-4eb1-b2d2-29606cc6549a')]}}\n",
            "\n",
            "--- Executing 'call_llm_node' (ReAct Mode) ---\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ResourceExhausted",
          "evalue": "429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n  quota_dimensions {\n    key: \"model\"\n    value: \"gemini-2.5-flash\"\n  }\n  quota_dimensions {\n    key: \"location\"\n    value: \"global\"\n  }\n  quota_value: 10\n}\n, links {\n  description: \"Learn more about Gemini API quotas\"\n  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n}\n, retry_delay {\n  seconds: 25\n}\n]",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mResourceExhausted\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-17-3512137596.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\n--- User Query: '{user_query_multi_location}' ---\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0mfinal_state_multi_location\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mapp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minitial_state_multi_location\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m'messages'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langgraph/pregel/__init__.py\u001b[0m in \u001b[0;36mstream\u001b[0;34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, checkpoint_during, debug, subgraphs)\u001b[0m\n\u001b[1;32m   2434\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0mtask\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatch_cached_writes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2435\u001b[0m                         \u001b[0mloop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_writes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrites\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcached\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2436\u001b[0;31m                     for _ in runner.tick(\n\u001b[0m\u001b[1;32m   2437\u001b[0m                         \u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrites\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2438\u001b[0m                         \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_timeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-17-3512137596.py\u001b[0m in \u001b[0;36mcall_llm_node\u001b[0;34m(state)\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0;31m# while bind_tools handles the *format* of the tool call JSON.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0mllm_chain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprompt\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mllm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind_tools\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtools\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mllm_chain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"messages\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmessages\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"LLM Response (from call_llm_node): {response}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/runnables/base.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   3045\u001b[0m                         \u001b[0minput_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3046\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3047\u001b[0;31m                         \u001b[0minput_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3048\u001b[0m         \u001b[0;31m# finish the root run\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3049\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/runnables/base.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   5429\u001b[0m         \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5430\u001b[0m     ) -> Output:\n\u001b[0;32m-> 5431\u001b[0;31m         return self.bound.invoke(\n\u001b[0m\u001b[1;32m   5432\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5433\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_merge_configs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_google_genai/chat_models.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, code_execution, stop, **kwargs)\u001b[0m\n\u001b[1;32m   1253\u001b[0m                 )\n\u001b[1;32m   1254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1255\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1257\u001b[0m     def _get_ls_params(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    370\u001b[0m         return cast(\n\u001b[1;32m    371\u001b[0m             \u001b[0;34m\"ChatGeneration\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 372\u001b[0;31m             self.generate_prompt(\n\u001b[0m\u001b[1;32m    373\u001b[0m                 \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    374\u001b[0m                 \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36mgenerate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    955\u001b[0m     ) -> LLMResult:\n\u001b[1;32m    956\u001b[0m         \u001b[0mprompt_messages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_messages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprompts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 957\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt_messages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    958\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    959\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0moverride\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    774\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    775\u001b[0m                 results.append(\n\u001b[0;32m--> 776\u001b[0;31m                     self._generate_with_cache(\n\u001b[0m\u001b[1;32m    777\u001b[0m                         \u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m                         \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36m_generate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m   1020\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_from_stream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1021\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_generate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"run_manager\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1022\u001b[0;31m             result = self._generate(\n\u001b[0m\u001b[1;32m   1023\u001b[0m                 \u001b[0mmessages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1024\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_google_genai/chat_models.py\u001b[0m in \u001b[0;36m_generate\u001b[0;34m(self, messages, stop, run_manager, tools, functions, safety_settings, tool_config, generation_config, cached_content, tool_choice, **kwargs)\u001b[0m\n\u001b[1;32m   1340\u001b[0m             \u001b[0mtool_choice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtool_choice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1341\u001b[0m         )\n\u001b[0;32m-> 1342\u001b[0;31m         response: GenerateContentResponse = _chat_with_retry(\n\u001b[0m\u001b[1;32m   1343\u001b[0m             \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1344\u001b[0m             \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_google_genai/chat_models.py\u001b[0m in \u001b[0;36m_chat_with_retry\u001b[0;34m(generation_method, **kwargs)\u001b[0m\n\u001b[1;32m    208\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_chat_with_retry\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tenacity/__init__.py\u001b[0m in \u001b[0;36mwrapped_f\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    336\u001b[0m             \u001b[0mcopy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m             \u001b[0mwrapped_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatistics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatistics\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 338\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    339\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mretry_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mWrappedFn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tenacity/__init__.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m         \u001b[0mretry_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRetryCallState\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretry_object\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mdo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretry_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretry_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDoAttempt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tenacity/__init__.py\u001b[0m in \u001b[0;36miter\u001b[0;34m(self, retry_state)\u001b[0m\n\u001b[1;32m    376\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    377\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0maction\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 378\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretry_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    379\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tenacity/__init__.py\u001b[0m in \u001b[0;36mexc_check\u001b[0;34m(rs)\u001b[0m\n\u001b[1;32m    418\u001b[0m                 \u001b[0mretry_exc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretry_error_cls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfut\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 420\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mretry_exc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    421\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mretry_exc\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mfut\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tenacity/__init__.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNoReturn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_attempt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfailed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_attempt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    447\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mCancelledError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mFINISHED\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/concurrent/futures/_base.py\u001b[0m in \u001b[0;36m__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    399\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    400\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 401\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    402\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    403\u001b[0m                 \u001b[0;31m# Break a reference cycle with the exception in self._exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tenacity/__init__.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    478\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDoAttempt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 480\u001b[0;31m                     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    481\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# noqa: B902\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    482\u001b[0m                     \u001b[0mretry_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_google_genai/chat_models.py\u001b[0m in \u001b[0;36m_chat_with_retry\u001b[0;34m(**kwargs)\u001b[0m\n\u001b[1;32m    206\u001b[0m             ) from e\n\u001b[1;32m    207\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_chat_with_retry\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_google_genai/chat_models.py\u001b[0m in \u001b[0;36m_chat_with_retry\u001b[0;34m(**kwargs)\u001b[0m\n\u001b[1;32m    190\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_chat_with_retry\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 192\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mgeneration_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    193\u001b[0m         \u001b[0;31m# Do not retry for these errors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFailedPrecondition\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/ai/generativelanguage_v1beta/services/generative_service/client.py\u001b[0m in \u001b[0;36mgenerate_content\u001b[0;34m(self, request, model, contents, retry, timeout, metadata)\u001b[0m\n\u001b[1;32m    866\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    867\u001b[0m         \u001b[0;31m# Send the request.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 868\u001b[0;31m         response = rpc(\n\u001b[0m\u001b[1;32m    869\u001b[0m             \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m             \u001b[0mretry\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretry\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/api_core/gapic_v1/method.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, timeout, retry, compression, *args, **kwargs)\u001b[0m\n\u001b[1;32m    129\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"compression\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompression\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/api_core/retry/retry_unary.py\u001b[0m in \u001b[0;36mretry_wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    292\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maximum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmultiplier\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_multiplier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m             )\n\u001b[0;32m--> 294\u001b[0;31m             return retry_target(\n\u001b[0m\u001b[1;32m    295\u001b[0m                 \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_predicate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/api_core/retry/retry_unary.py\u001b[0m in \u001b[0;36mretry_target\u001b[0;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m             \u001b[0;31m# defer to shared logic for handling errors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m             next_sleep = _retry_error_helper(\n\u001b[0m\u001b[1;32m    157\u001b[0m                 \u001b[0mexc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m                 \u001b[0mdeadline\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/api_core/retry/retry_base.py\u001b[0m in \u001b[0;36m_retry_error_helper\u001b[0;34m(exc, deadline, sleep_iterator, error_list, predicate_fn, on_error_fn, exc_factory_fn, original_timeout)\u001b[0m\n\u001b[1;32m    212\u001b[0m             \u001b[0moriginal_timeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         )\n\u001b[0;32m--> 214\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mfinal_exc\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msource_exc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    215\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mon_error_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0mon_error_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/api_core/retry/retry_unary.py\u001b[0m in \u001b[0;36mretry_target\u001b[0;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misawaitable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ASYNC_RETRY_WARNING\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/api_core/timeout.py\u001b[0m in \u001b[0;36mfunc_with_timeout\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    128\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"timeout\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mremaining_timeout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc_with_timeout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/api_core/grpc_helpers.py\u001b[0m in \u001b[0;36merror_remapped_callable\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mcallable_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mgrpc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRpcError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_grpc_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0merror_remapped_callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mResourceExhausted\u001b[0m: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n  quota_dimensions {\n    key: \"model\"\n    value: \"gemini-2.5-flash\"\n  }\n  quota_dimensions {\n    key: \"location\"\n    value: \"global\"\n  }\n  quota_value: 10\n}\n, links {\n  description: \"Learn more about Gemini API quotas\"\n  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n}\n, retry_delay {\n  seconds: 25\n}\n]"
          ]
        }
      ]
    }
  ]
}